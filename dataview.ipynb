{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade imbalanced-learn\n",
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 열 출력 수를 늘림\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.filter(like='V').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=30, figsize=(50, 25))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터 분할 및 교차검증\n",
    "# StratifiedKFold, Lgbm 모델로 교차검증 (amount 칼럼에만 스케일링 적용)\n",
    "# 데이터 분할을 교차검증으로 stratifiedkfold로 진행하고 lgbm 모델로 성능평가하고, 이후에 스케일링을 amount 칼럼만 진행해서 데이터 리킹 없이 진행\n",
    "# 스케일링 이후에 smote 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "numeric_features = ['Amount'] \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "# 스케일링 -> SMOTE 오버샘플링 -> LGBM 모델\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),   \n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=42)), \n",
    "    ('classifier', LGBMClassifier())\n",
    "])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = ['precision', 'recall', 'f1', 'average_precision']\n",
    "scores = cross_validate(pipeline, X, y, cv=skf, scoring=scoring)\n",
    "\n",
    "\n",
    "print(\"Cross-Validation Results:\")\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()} Scores:\", scores[f'test_{metric}'])\n",
    "    print(f\"Mean {metric.capitalize()}:\", scores[f'test_{metric}'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스케일링 유\n",
    "# class 0 이 정상, class 1 이 사기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 136418, number of negative: 136418\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272836, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 136418, number of negative: 136418\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272836, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 136418, number of negative: 136418\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272836, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 136419, number of negative: 136419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272838, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 136419, number of negative: 136419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272838, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "교차검증 결과 :\n",
      "Precision Scores: [0.67021277 0.73863636 0.7972973  0.7032967  0.63829787]\n",
      "Mean Precision: 0.7095482005056473\n",
      "Recall Scores: [0.875      0.90277778 0.81944444 0.88888889 0.83333333]\n",
      "Mean Recall: 0.8638888888888889\n",
      "F1 Scores: [0.75903614 0.8125     0.80821918 0.78527607 0.72289157]\n",
      "Mean F1: 0.7775845925090394\n",
      "Average_precision Scores: [0.8709761  0.90509129 0.76805151 0.84399137 0.78560469]\n",
      "Mean Average_precision: 0.8347429933781617\n",
      "[LightGBM] [Info] Number of positive: 170523, number of negative: 170523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 341046, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Test Set Predictions:\n",
      "Predicted Classes:\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# id와 time 칼럼 제거\n",
    "train = train.drop(columns=['id', 'Time'])\n",
    "test = test.drop(columns=['id', 'Time'])\n",
    "\n",
    "X_train = train.drop(columns=['Class'])\n",
    "y_train = train['Class']\n",
    "X_test = test\n",
    "\n",
    "needscaling_features = ['Amount'] \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), needscaling_features) \n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "# Scaling -> SMOTE -> LGBM \n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),   \n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=42)), \n",
    "    ('classifier', LGBMClassifier()) \n",
    "])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = ['precision', 'recall', 'f1', 'average_precision']\n",
    "\n",
    "cv_results = cross_validate(pipeline, X_train, y_train, cv=skf, scoring=scoring)\n",
    "\n",
    "print(\"교차검증 결과 :\")\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()} Scores:\", cv_results[f'test_{metric}'])\n",
    "    print(f\"Mean {metric.capitalize()}:\", cv_results[f'test_{metric}'].mean())\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Set Predictions:\")\n",
    "print(\"Predicted Classes:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스케일링 무"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 136418, number of negative: 136418\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272836, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 136418, number of negative: 136418\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272836, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 136418, number of negative: 136418\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272836, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 136419, number of negative: 136419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272838, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 136419, number of negative: 136419\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 272838, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "교차검증 결과 :\n",
      "Precision Scores: [0.77922078 0.77647059 0.78947368 0.7654321  0.81333333]\n",
      "Mean Precision: 0.784786096753073\n",
      "Recall Scores: [0.83333333 0.91666667 0.83333333 0.86111111 0.84722222]\n",
      "Mean Recall: 0.8583333333333334\n",
      "F1 Scores: [0.80536913 0.84076433 0.81081081 0.81045752 0.82993197]\n",
      "Mean F1: 0.8194667517333531\n",
      "Average_precision Scores: [0.83618519 0.92049719 0.66990629 0.80275151 0.79885199]\n",
      "Mean Average_precision: 0.8056384332305602\n",
      "[LightGBM] [Info] Number of positive: 170523, number of negative: 170523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7395\n",
      "[LightGBM] [Info] Number of data points in the train set: 341046, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Test Set Predictions:\n",
      "Predicted Classes:\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "train = train.drop(columns=['id', 'Time'])\n",
    "test = test.drop(columns=['id', 'Time'])\n",
    "\n",
    "X_train = train.drop(columns=['Class'])\n",
    "y_train = train['Class']\n",
    "X_test = test\n",
    "\n",
    "# SMOTE -> LGBM\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(sampling_strategy='minority', random_state=42)), \n",
    "    ('classifier', LGBMClassifier()) \n",
    "])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = ['precision', 'recall', 'f1', 'average_precision']\n",
    "\n",
    "cv_results = cross_validate(pipeline, X_train, y_train, cv=skf, scoring=scoring)\n",
    "\n",
    "print(\"교차검증 결과 :\")\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()} Scores:\", cv_results[f'test_{metric}'])\n",
    "    print(f\"Mean {metric.capitalize()}:\", cv_results[f'test_{metric}'].mean())\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Set Predictions:\")\n",
    "print(\"Predicted Classes:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 지표 공부 - 어떤 성능지표와 시각화 도구 -> 해석하는데 용이하게/ 매치 시켜서 생각하는게 아님 \"(ex 혼동행렬은 다른거 같음)\"\n",
    "# smote를 적용한 데이터에서 각각의 성능 지표를 모델에 적용시킬때 유의점 파악 (ex. 훈련데이터에서만 적용되는 것임, 따라서 교차검증을 통해 검증데이터에서는 적용이 안되는거..?) -> 이건 스모트 내부에서 작용 \n",
    "# -> smote 작동원리 공부!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "\n",
    "# 전체 데이터로 최종 모델 학습\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# 테스트 데이터 분할 (예시용으로 전체 데이터를 훈련 후 테스트 세트로 분할하여 평가)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 최종 모델로 예측 확률 계산\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ROC Curve 계산\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# ROC Curve 그리기\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "\n",
    "# 설명: Precision-Recall Curve는 다양한 임곗값에서 **Precision(정밀도)**과 **Recall(재현율)**의 관계를 시각화한 그래프입니다.\n",
    "# 용도: 특히 불균형 데이터셋에서 모델의 성능을 평가하는 데 유용합니다. Precision이 높을수록 양성 예측의 정확도가 높고, Recall이 높을수록 실제 양성을 잘 찾아냅니다.\n",
    "# AUC-PR: 그래프 아래 면적(AUC)을 계산해 성능을 수치로 나타낼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = average_precision_score(y_test, y_prob)\n",
    "\n",
    "plt.plot(recall, precision, label=f'Precision-Recall Curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix (혼동 행렬)\n",
    "\n",
    "# 설명: Confusion Matrix는 모델의 예측 결과를 TP (True Positive), FP (False Positive), FN (False Negative), **TN (True Negative)**으로 나타낸 표입니다.\n",
    "# 용도: 모델의 예측 결과를 자세히 분석할 수 있어, 특정 오류(예: 1종 오류와 2종 오류)를 줄이는 데 유용합니다.\n",
    "# 시각화: heatmap으로 시각화하여 직관적으로 오류 분포를 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred Neg', 'Pred Pos'], yticklabels=['True Neg', 'True Pos'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Results:\n",
    "# Precision Scores: [0.67741935 0.73626374 0.81081081 0.67021277 0.6744186 ]\n",
    "# Mean Precision: 0.7138250545043733\n",
    "\n",
    "# Recall Scores: [0.875      0.93055556 0.83333333 0.875      0.80555556]\n",
    "# Mean Recall: 0.8638888888888889\n",
    "\n",
    "# F1 Scores: [0.76363636 0.82208589 0.82191781 0.75903614 0.73417722]\n",
    "# Mean F1: 0.7801706842388562\n",
    "\n",
    "# Average_precision Scores: [0.83686128 0.91986044 0.81887023 0.78667227 0.80993289]\n",
    "# Mean Average_precision: 0.8344394228496679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 처리 진행 안함\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for column in df.select_dtypes(include='number').columns:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.boxplot(df[column])\n",
    "#     plt.title(f'Boxplot of {column}')\n",
    "#     plt.xlabel(column)\n",
    "#     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
